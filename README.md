# DialSummEval: Revisiting summarization evaluation for dialogues

The human judgments we used are under `annotation_campus`, you can also download them through [Link](https://drive.google.com/file/d/1LW1Ii_exspexg1oJTfrU8B4Bmzf2nJl-/view).  

The annotations from AMT we did not use are under `annotation_AMT`.  

The code under `./reproduce/metrics` is used to compute the values of metrics and the files under `./reproduce/analysis/models_eval_new` record the outputs of the metrics. `analysis.py` contains some functions about analysis, such as the correlation calculation. 


